{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Courses    Fee Duration  Discount\n",
      "0    Spark  22000   30days      1000\n",
      "1  PySpark  25000   50days      2300\n",
      "2   Hadoop  23000   55days      1000\n",
      "3   Python  24000   40days      1200\n",
      "4   Pandas  26000   60days      2500\n",
      "5   Hadoop  25000   35days      1300\n",
      "6    Spark  25000   55days      1400\n",
      "7   Python  22000   50days      1600\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "technologies   = ({\n",
    "    'Courses':[\"Spark\",\"PySpark\",\"Hadoop\",\"Python\",\"Pandas\",\"Hadoop\",\"Spark\",\"Python\"],\n",
    "    'Fee' :[22000,25000,23000,24000,26000,25000,25000,22000],\n",
    "    'Duration':['30days','50days','55days','40days','60days','35days','55days','50days'],\n",
    "    'Discount':[1000,2300,1000,1200,2500,1300,1400,1600]\n",
    "                })\n",
    "df = pd.DataFrame(technologies, columns=['Courses','Fee','Duration','Discount'])\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Fee  Discount\n",
      "Courses                 \n",
      "Hadoop   48000      2300\n",
      "Pandas   26000      2500\n",
      "PySpark  25000      2300\n",
      "Python   46000      2800\n",
      "Spark    47000      2400\n"
     ]
    }
   ],
   "source": [
    "# Use GroupBy() to compute the sum\n",
    "df2 = df.groupby('Courses').sum()\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Courses\n",
      "Hadoop     48000\n",
      "Pandas     26000\n",
      "PySpark    25000\n",
      "Python     46000\n",
      "Spark      47000\n",
      "Name: Fee, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Use GroupBy() & compute sum on specific column\n",
    "df2 = df.groupby('Courses')['Fee'].sum()\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Courses  Duration\n",
      "Hadoop   35days      25000\n",
      "         55days      23000\n",
      "Pandas   60days      26000\n",
      "PySpark  50days      25000\n",
      "Python   40days      24000\n",
      "         50days      22000\n",
      "Spark    30days      22000\n",
      "         55days      25000\n",
      "Name: Fee, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Using GroupBy multiple column\n",
    "df2 = df.groupby(['Courses','Duration'])['Fee'].sum()\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           sum  count\n",
      "Courses              \n",
      "Hadoop   48000      2\n",
      "Pandas   26000      1\n",
      "PySpark  25000      1\n",
      "Python   46000      2\n",
      "Spark    47000      2\n",
      "           Fee      \n",
      "           sum count\n",
      "Courses             \n",
      "Hadoop   48000     2\n",
      "Pandas   26000     1\n",
      "PySpark  25000     1\n",
      "Python   46000     2\n",
      "Spark    47000     2\n"
     ]
    }
   ],
   "source": [
    "# Groupby and get sum() and count()\n",
    "df2 = df.groupby('Courses')['Fee'].agg(['sum','count'])\n",
    "print(df2)\n",
    "\n",
    "# Pandas groupby get sum() and count()\n",
    "df2 = df.groupby('Courses').agg({'Fee': ['sum','count']})\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Fee  Discount\n",
      "Courses                 \n",
      "Spark    47000      2400\n",
      "PySpark  25000      2300\n",
      "Hadoop   48000      2300\n",
      "Python   46000      2800\n",
      "Pandas   26000      2500\n"
     ]
    }
   ],
   "source": [
    "# Remove sorting on grouped results\n",
    "df2=df.groupby(by=['Courses'], sort=False).sum()\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Fee  Discount\n",
      "Courses                 \n",
      "Spark    47000      2400\n",
      "Python   46000      2800\n",
      "PySpark  25000      2300\n",
      "Pandas   26000      2500\n",
      "Hadoop   48000      2300\n"
     ]
    }
   ],
   "source": [
    "# Sorting group keys on descending order\n",
    "groupedDF = df.groupby('Courses',sort=False).sum()\n",
    "sortedDF=groupedDF.sort_values('Courses', ascending=False)\n",
    "print(sortedDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Courses</th>\n",
       "      <th>Fee</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Discount</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Courses</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Hadoop</th>\n",
       "      <th>2</th>\n",
       "      <td>Hadoop</td>\n",
       "      <td>23000</td>\n",
       "      <td>55days</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Hadoop</td>\n",
       "      <td>25000</td>\n",
       "      <td>35days</td>\n",
       "      <td>1300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pandas</th>\n",
       "      <th>4</th>\n",
       "      <td>Pandas</td>\n",
       "      <td>26000</td>\n",
       "      <td>60days</td>\n",
       "      <td>2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PySpark</th>\n",
       "      <th>1</th>\n",
       "      <td>PySpark</td>\n",
       "      <td>25000</td>\n",
       "      <td>50days</td>\n",
       "      <td>2300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Python</th>\n",
       "      <th>7</th>\n",
       "      <td>Python</td>\n",
       "      <td>22000</td>\n",
       "      <td>50days</td>\n",
       "      <td>1600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Python</td>\n",
       "      <td>24000</td>\n",
       "      <td>40days</td>\n",
       "      <td>1200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Spark</th>\n",
       "      <th>0</th>\n",
       "      <td>Spark</td>\n",
       "      <td>22000</td>\n",
       "      <td>30days</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Spark</td>\n",
       "      <td>25000</td>\n",
       "      <td>55days</td>\n",
       "      <td>1400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Courses    Fee Duration  Discount\n",
       "Courses                                     \n",
       "Hadoop  2   Hadoop  23000   55days      1000\n",
       "        5   Hadoop  25000   35days      1300\n",
       "Pandas  4   Pandas  26000   60days      2500\n",
       "PySpark 1  PySpark  25000   50days      2300\n",
       "Python  7   Python  22000   50days      1600\n",
       "        3   Python  24000   40days      1200\n",
       "Spark   0    Spark  22000   30days      1000\n",
       "        6    Spark  25000   55days      1400"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('Courses').apply(lambda x: x.sort_values('Fee'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Courses    Fee\n",
      "0   Hadoop  48000\n",
      "1   Pandas  26000\n",
      "2  PySpark  25000\n",
      "3   Python  46000\n",
      "4    Spark  47000\n"
     ]
    }
   ],
   "source": [
    "# Using as_index=False\n",
    "df2 = df.groupby('Courses', as_index =False)['Fee'].sum()\n",
    "\n",
    "# Using reset_index()\n",
    "df2 = df.groupby(['Courses'])['Fee'].sum().reset_index()\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Courses  Duration\n",
      "Hadoop   35days      1300\n",
      "         55days      1000\n",
      "Pandas   60days      2500\n",
      "PySpark  50days      2300\n",
      "Python   40days      1200\n",
      "         50days      1600\n",
      "Spark    30days      1000\n",
      "         55days      1400\n",
      "Name: Discount, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# GroupBy multiple columns using agg()\n",
    "df2 = df.groupby(['Courses','Duration'])['Discount'].agg(\"sum\")\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1000\n",
      "1    2300\n",
      "2    1000\n",
      "3    1200\n",
      "4    2500\n",
      "5    1300\n",
      "6    1400\n",
      "7    1600\n",
      "Name: Discount, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# GroupBy multiple columns using transform()\n",
    "df2 = df.groupby(['Courses', 'Fee'])['Discount'].transform('sum')\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Fee                                              Discount  \\\n",
      "Duration   30days   35days   40days   50days   55days   60days   30days   \n",
      "Courses                                                                   \n",
      "Hadoop        0.0  25000.0      0.0      0.0  23000.0      0.0      0.0   \n",
      "Pandas        0.0      0.0      0.0      0.0      0.0  26000.0      0.0   \n",
      "PySpark       0.0      0.0      0.0  25000.0      0.0      0.0      0.0   \n",
      "Python        0.0      0.0  24000.0  22000.0      0.0      0.0      0.0   \n",
      "Spark     22000.0      0.0      0.0      0.0  25000.0      0.0   1000.0   \n",
      "\n",
      "                                                  \n",
      "Duration  35days  40days  50days  55days  60days  \n",
      "Courses                                           \n",
      "Hadoop    1300.0     0.0     0.0  1000.0     0.0  \n",
      "Pandas       0.0     0.0     0.0     0.0  2500.0  \n",
      "PySpark      0.0     0.0  2300.0     0.0     0.0  \n",
      "Python       0.0  1200.0  1600.0     0.0     0.0  \n",
      "Spark        0.0     0.0     0.0  1400.0     0.0  \n"
     ]
    }
   ],
   "source": [
    "# GroupBy multiple columns using pivot function\n",
    "df2 = df.groupby(['Courses','Duration'],as_index = False).sum().pivot('Courses','Duration').fillna(0)\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Fee  Discount\n",
      "Courses Duration                 \n",
      "Spark   30days    22000      1000\n",
      "PySpark 50days    25000      2300\n",
      "Hadoop  55days    23000      1000\n",
      "Python  40days    24000      1200\n",
      "Pandas  60days    26000      2500\n",
      "Hadoop  35days    25000      1300\n",
      "Spark   55days    25000      1400\n",
      "Python  50days    22000      1600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Darshi\\AppData\\Local\\Temp\\ipykernel_11428\\2898248389.py:2: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  df2 = df.set_index(['Courses','Duration']).sum(level=[0,1])\n"
     ]
    }
   ],
   "source": [
    "# DataFrame.set_index using sum with level\n",
    "df2 = df.set_index(['Courses','Duration']).sum(level=[0,1])\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Courses  Duration\n",
      "Hadoop   35days      1300\n",
      "         55days      1000\n",
      "Pandas   60days      2500\n",
      "PySpark  50days      2300\n",
      "Python   40days      1200\n",
      "         50days      1600\n",
      "Spark    30days      1000\n",
      "         55days      1400\n",
      "Name: Discount, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# GroupBy multiple columns using agg()\n",
    "df2 = df.groupby(['Courses','Duration'])['Discount'].agg(\"sum\")\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Courses  Duration\n",
      "Hadoop   35days      1300\n",
      "         55days      1000\n",
      "Pandas   60days      2500\n",
      "PySpark  50days      2300\n",
      "Python   40days      1200\n",
      "         50days      1600\n",
      "Spark    30days      1000\n",
      "         55days      1400\n",
      "Name: Discount, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# GroupBy multiple columns using agg()\n",
    "df2 = df.groupby(['Courses','Duration'])['Discount'].sum()\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
